{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8732fb1",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6cca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc29e39",
   "metadata": {},
   "source": [
    "#### Methods for sentiment analysis\n",
    "\n",
    "1.Logistic Regression or Naive Bayes\n",
    "\n",
    "2.Simple RNNs or LSTM Neural Networks\n",
    "\n",
    "3.Transformers and Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4b390",
   "metadata": {},
   "source": [
    "#### Encodings\n",
    "\n",
    "1.CountVectorizer(Bag of words) and TFIDFtransformer\n",
    "\n",
    "2.Word2Vec\n",
    "\n",
    "3.Word Embedding\n",
    "\n",
    "4.Bert Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13e8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv',usecols = ['v1','v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6501b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['cateogry','message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123a3f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cateogry</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cateogry                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436c7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['cateogry'].apply(lambda x:1 if x=='spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f6e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cateogry</th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cateogry                                            message  class\n",
       "0      ham  Go until jurong point, crazy.. Available only ...      0\n",
       "1      ham                      Ok lar... Joking wif u oni...      0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...      1\n",
       "3      ham  U dun say so early hor... U c already then say...      0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2c9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('cateogry',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6628260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  class\n",
       "0  Go until jurong point, crazy.. Available only ...      0\n",
       "1                      Ok lar... Joking wif u oni...      0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...      1\n",
       "3  U dun say so early hor... U c already then say...      0\n",
       "4  Nah I don't think he goes to usf, he lives aro...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48273278",
   "metadata": {},
   "source": [
    "##### Balancing the unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23128642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "class                                                                       \n",
       "0        4825   4516                             Sorry, I'll call later   30\n",
       "1         747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('class').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68fc0534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message    4825\n",
       "class      4825\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['class']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30602f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message    747\n",
       "class      747\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['class']==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb23cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ham = df[df['class']==0]\n",
    "df_spam = df[df['class']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cedbe699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07111500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ham_final = df_ham.sample(df_spam.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6df7761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>See? I thought it all through</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Home so we can always chat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>Oh unintentionally not bad timing. Great. Fing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>Good evening! this is roger. How are you?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>Very hurting n meaningful lines ever: \\I compr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                message  class\n",
       "2174                      See? I thought it all through      0\n",
       "211                          Home so we can always chat      0\n",
       "5146  Oh unintentionally not bad timing. Great. Fing...      0\n",
       "3119          Good evening! this is roger. How are you?      0\n",
       "1669  Very hurting n meaningful lines ever: \\I compr...      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ham_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f341e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_ham_final,df_spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a816342c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>See? I thought it all through</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Home so we can always chat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>Oh unintentionally not bad timing. Great. Fing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>Good evening! this is roger. How are you?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>Very hurting n meaningful lines ever: \\I compr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                message  class\n",
       "2174                      See? I thought it all through      0\n",
       "211                          Home so we can always chat      0\n",
       "5146  Oh unintentionally not bad timing. Great. Fing...      0\n",
       "3119          Good evening! this is roger. How are you?      0\n",
       "1669  Very hurting n meaningful lines ever: \\I compr...      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559fccd",
   "metadata": {},
   "source": [
    "##### Count Vectorizer and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "091d4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a57e37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad2ffa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['ram is a good boy','ravi is a good boy','she is a good girl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea995d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9113694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60d1bb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boy', 'girl', 'good', 'is', 'ram', 'ravi', 'she']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78868d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "test = 'she is not santa claus'\n",
    "test_final = cv.transform([test])\n",
    "print(test_final.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89ef8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e9ffe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tffinal = tf.fit_transform(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bba7986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50410689, 0.        , 0.39148397, 0.39148397, 0.66283998,\n",
       "        0.        , 0.        ],\n",
       "       [0.50410689, 0.        , 0.39148397, 0.39148397, 0.        ,\n",
       "        0.66283998, 0.        ],\n",
       "       [0.        , 0.6088451 , 0.35959372, 0.35959372, 0.        ,\n",
       "        0.        , 0.6088451 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tffinal.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83909e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tftest = tf.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2038b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.50854232, 0.        ,\n",
       "        0.        , 0.861037  ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tftest.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeca53c",
   "metadata": {},
   "source": [
    "##### Cleaning the Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c70db01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saibh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5d28d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(list(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "057636a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fd0470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_sample = df_final.iloc[0]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15f01530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See? I thought it all through'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4977bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(message_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48e9c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04ec3bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['See', '?', 'I', 'thought']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43095002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "080d4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "puncs = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dadf66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7cb1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in tokens if w not in puncs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1bd19479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['See', 'I', 'thought']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfd55df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0231cb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See I thought'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45694ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaning(col):\n",
    "    tokens = nltk.word_tokenize(col)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [w for w in tokens if w not in puncs]\n",
    "    tweet = ' '.join(tokens)\n",
    "    #removing old style RT tweets\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    #remove hastags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    #remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    #lower casing\n",
    "    tweet = tweet.lower()\n",
    "    #only to keep data from a-z\n",
    "    tweet = re.sub(r'[^(a-zA-z)\\s]','',tweet)\n",
    "    return tweet    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8186a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['clean_messages'] = df_final['message'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d97ec97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "      <th>clean_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>See? I thought it all through</td>\n",
       "      <td>0</td>\n",
       "      <td>see i thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Home so we can always chat</td>\n",
       "      <td>0</td>\n",
       "      <td>home always chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>Oh unintentionally not bad timing. Great. Fing...</td>\n",
       "      <td>0</td>\n",
       "      <td>oh unintentionally bad timing great fingers tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>Good evening! this is roger. How are you?</td>\n",
       "      <td>0</td>\n",
       "      <td>good evening roger how</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>Very hurting n meaningful lines ever: \\I compr...</td>\n",
       "      <td>0</td>\n",
       "      <td>very hurting n meaningful lines ever \\i compro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                message  class  \\\n",
       "2174                      See? I thought it all through      0   \n",
       "211                          Home so we can always chat      0   \n",
       "5146  Oh unintentionally not bad timing. Great. Fing...      0   \n",
       "3119          Good evening! this is roger. How are you?      0   \n",
       "1669  Very hurting n meaningful lines ever: \\I compr...      0   \n",
       "\n",
       "                                         clean_messages  \n",
       "2174                                      see i thought  \n",
       "211                                    home always chat  \n",
       "5146  oh unintentionally bad timing great fingers tr...  \n",
       "3119                             good evening roger how  \n",
       "1669  very hurting n meaningful lines ever \\i compro...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "285f4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "Tfidf_Trans = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b810c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.fit(df_final['clean_messages'])\n",
    "Final_messages = count_vect.transform(df_final['clean_messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6863cd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_messages.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff9eaa85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['____',\n",
       " 'aaooooright',\n",
       " 'ab',\n",
       " 'abdomen',\n",
       " 'aberdeen',\n",
       " 'abi',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'about',\n",
       " 'abroad',\n",
       " 'abt',\n",
       " 'abta',\n",
       " 'abuse',\n",
       " 'abusers',\n",
       " 'ac',\n",
       " 'acc',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accommodation',\n",
       " 'accommodationvouchers',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'ache',\n",
       " 'acid',\n",
       " 'aclpm',\n",
       " 'aco',\n",
       " 'across',\n",
       " 'acsmsrewards',\n",
       " 'actin',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'actor',\n",
       " 'actually',\n",
       " 'acwicmbcktzr',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'addamsfa',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addie',\n",
       " 'address',\n",
       " 'admirer',\n",
       " 'admission',\n",
       " 'adore',\n",
       " 'adp',\n",
       " 'adrian',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'adventuring',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advisors',\n",
       " 'ae',\n",
       " 'affection',\n",
       " 'afraid',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'ag',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'ageperwksub',\n",
       " 'ageppermesssubscription',\n",
       " 'ages',\n",
       " 'agidhane',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreen',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahmad',\n",
       " 'ahthe',\n",
       " 'ai',\n",
       " 'aight',\n",
       " 'aint',\n",
       " 'airport',\n",
       " 'airtel',\n",
       " 'aiya',\n",
       " 'aiyo',\n",
       " 'aj',\n",
       " 'akonlonely',\n",
       " 'al',\n",
       " 'alaikkumpride',\n",
       " 'alert',\n",
       " 'alertfrom',\n",
       " 'alerts',\n",
       " 'alex',\n",
       " 'alfie',\n",
       " 'algarve',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'alrite',\n",
       " 'also',\n",
       " 'alsoor',\n",
       " 'altocoukwavewaveasp',\n",
       " 'alwa',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amanda',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'amigos',\n",
       " 'amk',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amp',\n",
       " 'ampm',\n",
       " 'amt',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'and',\n",
       " 'angry',\n",
       " 'animation',\n",
       " 'anna',\n",
       " 'anniversary',\n",
       " 'annoncement',\n",
       " 'announcement',\n",
       " 'anonymous',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'ansr',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answerin',\n",
       " 'answering',\n",
       " 'answr',\n",
       " 'antelope',\n",
       " 'antibiotic',\n",
       " 'any',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'aom',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apologetic',\n",
       " 'app',\n",
       " 'apply',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appropriate',\n",
       " 'approx',\n",
       " 'appy',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'aptitude',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arcade',\n",
       " 'ard',\n",
       " 'are',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'aries',\n",
       " 'armand',\n",
       " 'arng',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrow',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'artists',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'asapok',\n",
       " 'ashes',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asshole',\n",
       " 'assumed',\n",
       " 'at',\n",
       " 'athome',\n",
       " 'atlast',\n",
       " 'attached',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attributed',\n",
       " 'auction',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'aunty',\n",
       " 'australia',\n",
       " 'auto',\n",
       " 'availa',\n",
       " 'available',\n",
       " 'availablethey',\n",
       " 'avble',\n",
       " 'ave',\n",
       " 'avenge',\n",
       " 'avoid',\n",
       " 'await',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'aww',\n",
       " 'ax',\n",
       " 'axis',\n",
       " 'ay',\n",
       " 'baaaaaaaabe',\n",
       " 'babe',\n",
       " 'babes',\n",
       " 'baby',\n",
       " 'babygoodbye',\n",
       " 'back',\n",
       " 'backdoor',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'badly',\n",
       " 'badrith',\n",
       " 'bahamas',\n",
       " 'bailiff',\n",
       " 'bak',\n",
       " 'bakra',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'band',\n",
       " 'bangb',\n",
       " 'bangbabes',\n",
       " 'bank',\n",
       " 'banned',\n",
       " 'banneduk',\n",
       " 'bannfwflyppm',\n",
       " 'barbie',\n",
       " 'bari',\n",
       " 'barkleys',\n",
       " 'barry',\n",
       " 'basket',\n",
       " 'bat',\n",
       " 'batchlor',\n",
       " 'battery',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bbc',\n",
       " 'bbdeluxe',\n",
       " 'bblue',\n",
       " 'bc',\n",
       " 'bcm',\n",
       " 'bcmsfwcnxx',\n",
       " 'bcmwcnxx',\n",
       " 'bcoz',\n",
       " 'bcums',\n",
       " 'bday',\n",
       " 'be',\n",
       " 'beads',\n",
       " 'bears',\n",
       " 'beautiful',\n",
       " 'bec',\n",
       " 'because',\n",
       " 'becausethey',\n",
       " 'become',\n",
       " 'becoz',\n",
       " 'bed',\n",
       " 'bedrm',\n",
       " 'bedroom',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforewent',\n",
       " 'beg',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'behave',\n",
       " 'believe',\n",
       " 'bellearlier',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'beneath',\n",
       " 'benefits',\n",
       " 'bergkamp',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'bettr',\n",
       " 'beverage',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bid',\n",
       " 'bids',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bill',\n",
       " 'billed',\n",
       " 'billing',\n",
       " 'billion',\n",
       " 'bin',\n",
       " 'biola',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'biro',\n",
       " 'birthdate',\n",
       " 'birthday',\n",
       " 'bishan',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'biz',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'blankets',\n",
       " 'blessings',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'bloke',\n",
       " 'blonde',\n",
       " 'bloodsend',\n",
       " 'bloomberg',\n",
       " 'bloombergcom',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blu',\n",
       " 'blue',\n",
       " 'bluetooth',\n",
       " 'bluetoothhdset',\n",
       " 'blur',\n",
       " 'bmw',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'bold',\n",
       " 'boltblue',\n",
       " 'bone',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'booking',\n",
       " 'bookmark',\n",
       " 'books',\n",
       " 'booty',\n",
       " 'bootydelious',\n",
       " 'borderline',\n",
       " 'bored',\n",
       " 'borin',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'both',\n",
       " 'bother',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boxcpm',\n",
       " 'boxnqp',\n",
       " 'boxqu',\n",
       " 'boxskch',\n",
       " 'boxskwpppm',\n",
       " 'boxwrc',\n",
       " 'boy',\n",
       " 'boye',\n",
       " 'boys',\n",
       " 'boytoy',\n",
       " 'brah',\n",
       " 'brainy',\n",
       " 'brand',\n",
       " 'bray',\n",
       " 'brb',\n",
       " 'break',\n",
       " 'breaker',\n",
       " 'breaks',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'bright',\n",
       " 'brilliant',\n",
       " 'brin',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'british',\n",
       " 'britney',\n",
       " 'bro',\n",
       " 'bros',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'browse',\n",
       " 'browsin',\n",
       " 'bslvyl',\n",
       " 'bsnl',\n",
       " 'bt',\n",
       " 'btnational',\n",
       " 'btnationalrate',\n",
       " 'btooth',\n",
       " 'btwn',\n",
       " 'bu',\n",
       " 'bucks',\n",
       " 'buddys',\n",
       " 'buff',\n",
       " 'buffy',\n",
       " 'bugis',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bulbs',\n",
       " 'bundle',\n",
       " 'buns',\n",
       " 'burger',\n",
       " 'burgundy',\n",
       " 'burn',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'burnt',\n",
       " 'burrito',\n",
       " 'bus',\n",
       " 'buses',\n",
       " 'busetop',\n",
       " 'busty',\n",
       " 'busy',\n",
       " 'busyi',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'buy',\n",
       " 'buyers',\n",
       " 'bx',\n",
       " 'bxipwe',\n",
       " 'by',\n",
       " 'byatch',\n",
       " 'bye',\n",
       " 'byleafcutter',\n",
       " 'ca',\n",
       " 'cab',\n",
       " 'cake',\n",
       " 'cakes',\n",
       " 'cal',\n",
       " 'call',\n",
       " 'callback',\n",
       " 'callcost',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'callfreefone',\n",
       " 'calling',\n",
       " 'calloptout',\n",
       " 'calloptoutfq',\n",
       " 'calloptouthf',\n",
       " 'calloptoutj',\n",
       " 'calloptoutjq',\n",
       " 'calloptoutlf',\n",
       " 'calloptoutndx',\n",
       " 'calloptoutqf',\n",
       " 'calls',\n",
       " 'callsminmobsmore',\n",
       " 'callsminmobsmorelkpoboxhpfl',\n",
       " 'callsminmoremobsemspoboxpowa',\n",
       " 'callsppm',\n",
       " 'calm',\n",
       " 'cam',\n",
       " 'camcorder',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'cameravideo',\n",
       " 'campus',\n",
       " 'can',\n",
       " 'canada',\n",
       " 'canary',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'canname',\n",
       " 'cant',\n",
       " 'capital',\n",
       " 'cappuccino',\n",
       " 'captain',\n",
       " 'captaining',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cardiff',\n",
       " 'care',\n",
       " 'careabout',\n",
       " 'cared',\n",
       " 'career',\n",
       " 'careers',\n",
       " 'careful',\n",
       " 'carlie',\n",
       " 'carlos',\n",
       " 'caroline',\n",
       " 'cars',\n",
       " 'cartoon',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cashbalance',\n",
       " 'cashbincouk',\n",
       " 'cashin',\n",
       " 'cashto',\n",
       " 'cast',\n",
       " 'casting',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catching',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'causing',\n",
       " 'cc',\n",
       " 'ccpmin',\n",
       " 'cd',\n",
       " 'cdgt',\n",
       " 'cds',\n",
       " 'celeb',\n",
       " 'celebrations',\n",
       " 'cell',\n",
       " 'center',\n",
       " 'centre',\n",
       " 'certificate',\n",
       " 'cfcaa',\n",
       " 'cha',\n",
       " 'chad',\n",
       " 'challenge',\n",
       " 'champneys',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'channel',\n",
       " 'chaps',\n",
       " 'character',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'charges',\n",
       " 'charity',\n",
       " 'charles',\n",
       " 'chart',\n",
       " 'charts',\n",
       " 'chase',\n",
       " 'chasing',\n",
       " 'chat',\n",
       " 'chatlines',\n",
       " 'chatter',\n",
       " 'chatting',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'cheat',\n",
       " 'cheating',\n",
       " 'chechi',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'cheer',\n",
       " 'cheers',\n",
       " 'chennai',\n",
       " 'chest',\n",
       " 'cheyyamo',\n",
       " 'chg',\n",
       " 'chgs',\n",
       " 'chik',\n",
       " 'chikku',\n",
       " 'child',\n",
       " 'childish',\n",
       " 'childporn',\n",
       " 'children',\n",
       " 'chillaxin',\n",
       " 'chinese',\n",
       " 'chinnu',\n",
       " 'chip',\n",
       " 'chitchat',\n",
       " 'chloe',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chords',\n",
       " 'chosen',\n",
       " 'chrgd',\n",
       " 'christians',\n",
       " 'christmas',\n",
       " 'cine',\n",
       " 'cinema',\n",
       " 'city',\n",
       " 'claim',\n",
       " 'claimcode',\n",
       " 'claims',\n",
       " 'claire',\n",
       " 'clash',\n",
       " 'class',\n",
       " 'classic',\n",
       " 'cld',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'cleared',\n",
       " 'clearing',\n",
       " 'click',\n",
       " 'cliff',\n",
       " 'clip',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closingdate',\n",
       " 'club',\n",
       " 'clubmobilescom',\n",
       " 'clubsaisai',\n",
       " 'cm',\n",
       " 'cmon',\n",
       " 'cncl',\n",
       " 'cnl',\n",
       " 'cnn',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'coat',\n",
       " 'coaxing',\n",
       " 'cocacola',\n",
       " 'cock',\n",
       " 'code',\n",
       " 'codexx',\n",
       " 'coffee',\n",
       " 'coimbatore',\n",
       " 'coincidence',\n",
       " 'colany',\n",
       " 'colin',\n",
       " 'colleagues',\n",
       " 'collect',\n",
       " 'collected',\n",
       " 'collection',\n",
       " 'colleg',\n",
       " 'colour',\n",
       " 'colourred',\n",
       " 'colours',\n",
       " 'com',\n",
       " 'combine',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'common',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'compare',\n",
       " 'compensation',\n",
       " 'competition',\n",
       " 'complain',\n",
       " 'complaint',\n",
       " 'complementary',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'complimentary',\n",
       " 'compulsory',\n",
       " 'computer',\n",
       " 'comuk',\n",
       " 'comukcm',\n",
       " 'conacted',\n",
       " 'concentrate',\n",
       " 'concert',\n",
       " 'condition',\n",
       " 'conditions',\n",
       " 'conference',\n",
       " 'confirm',\n",
       " 'confirmd',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'connect',\n",
       " 'connected',\n",
       " 'connection',\n",
       " 'consent',\n",
       " 'considering',\n",
       " 'console',\n",
       " 'contact',\n",
       " 'contacted',\n",
       " 'contains',\n",
       " 'content',\n",
       " 'contention',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'contract',\n",
       " 'control',\n",
       " 'cooked',\n",
       " 'cool',\n",
       " 'coolmob',\n",
       " 'cops',\n",
       " 'corect',\n",
       " 'cornwall',\n",
       " 'corporation',\n",
       " 'correct',\n",
       " 'corvettes',\n",
       " 'cos',\n",
       " 'cost',\n",
       " 'costa',\n",
       " 'costing',\n",
       " 'costmax',\n",
       " 'costpm',\n",
       " 'costs',\n",
       " 'cougarpen',\n",
       " 'cough',\n",
       " 'could',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'coz',\n",
       " 'cr',\n",
       " 'crab',\n",
       " 'cramps',\n",
       " 'crash',\n",
       " 'crave',\n",
       " 'crazy',\n",
       " 'crazyin',\n",
       " 'crbt',\n",
       " 'cream',\n",
       " 'created',\n",
       " 'creative',\n",
       " 'cred',\n",
       " 'credit',\n",
       " 'credited',\n",
       " 'credits',\n",
       " 'cres',\n",
       " 'cribbs',\n",
       " 'cro',\n",
       " 'cross',\n",
       " 'crossing',\n",
       " 'crowd',\n",
       " 'croydon',\n",
       " 'cruise',\n",
       " 'cs',\n",
       " 'csbcmwcnxx',\n",
       " 'csbcmwcnxxcallcostppmmobilesvary',\n",
       " 'csc',\n",
       " 'csh',\n",
       " 'csstop',\n",
       " 'cst',\n",
       " 'cts',\n",
       " 'ctxt',\n",
       " 'cud',\n",
       " 'cuddling',\n",
       " 'cum',\n",
       " 'cumin',\n",
       " 'cumming',\n",
       " 'cup',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'cust',\n",
       " 'custcare',\n",
       " 'customer',\n",
       " 'customercare',\n",
       " 'customers',\n",
       " 'customersqueries',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'cwwx',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'dai',\n",
       " 'daily',\n",
       " 'dance',\n",
       " 'dangerous',\n",
       " 'dare',\n",
       " 'dark',\n",
       " 'darkest',\n",
       " 'darlin',\n",
       " 'darling',\n",
       " 'dartboard',\n",
       " 'dat',\n",
       " 'date',\n",
       " 'dateboxessexcmxn',\n",
       " 'dates',\n",
       " 'dating',\n",
       " 'dave',\n",
       " 'day',\n",
       " 'daylove',\n",
       " 'days',\n",
       " 'daysn',\n",
       " 'daytime',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dealing',\n",
       " 'deals',\n",
       " 'deam',\n",
       " 'dear',\n",
       " 'dearloving',\n",
       " 'dearly',\n",
       " 'death',\n",
       " 'december',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'deck',\n",
       " 'deep',\n",
       " 'deepest',\n",
       " 'def',\n",
       " 'defer',\n",
       " 'definitely',\n",
       " 'definitly',\n",
       " 'del',\n",
       " 'delete',\n",
       " 'delhi',\n",
       " 'deliver',\n",
       " 'delivered',\n",
       " 'deliveredtomorrow',\n",
       " 'delivery',\n",
       " 'deltomorrow',\n",
       " 'deluxe',\n",
       " 'dem',\n",
       " 'den',\n",
       " 'dena',\n",
       " 'dengra',\n",
       " 'denis',\n",
       " 'depends',\n",
       " 'depressed',\n",
       " 'dept',\n",
       " 'derek',\n",
       " 'derp',\n",
       " 'desperate',\n",
       " 'despite',\n",
       " 'dessert',\n",
       " 'detail',\n",
       " 'details',\n",
       " 'detroit',\n",
       " 'devils',\n",
       " 'dey',\n",
       " 'dial',\n",
       " 'dialling',\n",
       " 'dick',\n",
       " 'did',\n",
       " 'didnt',\n",
       " 'didntgive',\n",
       " 'die',\n",
       " 'died',\n",
       " 'diet',\n",
       " 'diff',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital',\n",
       " 'digits',\n",
       " 'dining',\n",
       " 'dinner',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'dirtiest',\n",
       " 'dirty',\n",
       " 'dis',\n",
       " 'disagreeable',\n",
       " 'disaster',\n",
       " 'disconnect',\n",
       " 'discount',\n",
       " 'discreet',\n",
       " 'dislikes',\n",
       " 'ditto',\n",
       " 'divorce',\n",
       " 'diwali',\n",
       " 'dizzee',\n",
       " 'dload',\n",
       " 'dnt',\n",
       " 'do',\n",
       " 'dob',\n",
       " 'dobby',\n",
       " 'docs',\n",
       " 'doctor',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'doggin',\n",
       " 'dogging',\n",
       " 'dogs',\n",
       " 'doin',\n",
       " 'dointerested',\n",
       " 'doit',\n",
       " 'dollar',\n",
       " 'dollars',\n",
       " 'don',\n",
       " 'donate',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'dontcha',\n",
       " 'donyt',\n",
       " 'door',\n",
       " 'dorothy',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'doublemins',\n",
       " 'doubles',\n",
       " 'doubletxt',\n",
       " 'dough',\n",
       " 'down',\n",
       " 'download',\n",
       " 'downloaded',\n",
       " 'downloads',\n",
       " 'downon',\n",
       " 'dozens',\n",
       " 'dps',\n",
       " 'dracula',\n",
       " 'draw',\n",
       " 'draws',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drink',\n",
       " 'drinkin',\n",
       " 'drinks',\n",
       " 'drive',\n",
       " 'drivin',\n",
       " 'driving',\n",
       " 'drms',\n",
       " 'drop',\n",
       " 'dropped',\n",
       " 'drops',\n",
       " 'drove',\n",
       " 'drug',\n",
       " 'drunken',\n",
       " 'drvgsto',\n",
       " 'dry',\n",
       " 'dubsack',\n",
       " 'duchess',\n",
       " 'dude',\n",
       " 'due',\n",
       " 'dull',\n",
       " 'dump',\n",
       " 'dun',\n",
       " 'dunno',\n",
       " 'duvet',\n",
       " 'dvd',\n",
       " 'dwn',\n",
       " 'dysentry',\n",
       " 'ea',\n",
       " 'each',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earn',\n",
       " 'easier',\n",
       " 'eastenders',\n",
       " 'easter',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'eca',\n",
       " 'echo',\n",
       " 'ecstacy',\n",
       " 'educational',\n",
       " 'eek',\n",
       " 'eerie',\n",
       " 'efreefone',\n",
       " 'eg',\n",
       " 'egf',\n",
       " 'egg',\n",
       " 'eh',\n",
       " 'ehrr',\n",
       " 'eighth',\n",
       " 'eire',\n",
       " 'either',\n",
       " 'elaine',\n",
       " 'ello',\n",
       " 'else',\n",
       " 'elvis',\n",
       " 'em',\n",
       " 'email',\n",
       " 'embarrassed',\n",
       " 'embassy',\n",
       " 'emccouk',\n",
       " 'employee',\n",
       " 'en',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'ends',\n",
       " 'energy',\n",
       " 'eng',\n",
       " 'engin',\n",
       " 'england',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enough',\n",
       " 'enter',\n",
       " 'entered',\n",
       " 'entitled',\n",
       " 'entry',\n",
       " 'enufcredeit',\n",
       " 'envelope',\n",
       " 'envy',\n",
       " 'er',\n",
       " 'erm',\n",
       " 'erotic',\n",
       " 'err',\n",
       " 'error',\n",
       " 'ertini',\n",
       " 'escape',\n",
       " 'eshxxxxxxxxxxx',\n",
       " 'espe',\n",
       " 'especially',\n",
       " 'essay',\n",
       " ...]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e87ec02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_context = Tfidf_Trans.fit_transform(Final_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0db5f726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 3819)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_context.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b045fb0",
   "metadata": {},
   "source": [
    "##### Sampling the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e41e75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop('message',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7b43ed88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>clean_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0</td>\n",
       "      <td>all done all handed celebrations full swing yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>0</td>\n",
       "      <td>joy s father john then john ____ joy s father ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>0</td>\n",
       "      <td>whatever im pretty pissed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>0</td>\n",
       "      <td>happy birthday  may ur dreams come true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0</td>\n",
       "      <td>message text missing sender name missing numbe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                     clean_messages\n",
       "279       0    all done all handed celebrations full swing yet\n",
       "3608      0  joy s father john then john ____ joy s father ...\n",
       "1567      0                          whatever im pretty pissed\n",
       "1293      0           happy birthday  may ur dreams come true \n",
       "409       0  message text missing sender name missing numbe..."
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4288735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df_final['clean_messages'],df_final['class'],test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f84c497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer()\n",
    "TF = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "db5930e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_cv = CV.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7b35231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_tf = TF.fit_transform(Final_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "18e4a00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1045, 3071)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_tf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "52e17038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4a1056e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0357451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a9f9d4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(Final_tf.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8c19bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_cv_test = CV.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "61e70fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_tf_test = TF.transform(Final_cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4b0244e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(Final_tf_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "349f6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "254693bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[220  20]\n",
      " [  6 203]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(preds,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e902343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       226\n",
      "           1       0.97      0.91      0.94       223\n",
      "\n",
      "    accuracy                           0.94       449\n",
      "   macro avg       0.94      0.94      0.94       449\n",
      "weighted avg       0.94      0.94      0.94       449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "48470248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my love how come took long leave zaher s i got words ym happy see sad left i miss'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "145811bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cv = CV.transform([X_train[910]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c11591e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tf = TF.transform(sample_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "28ad8aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(sample_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6166cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbca21",
   "metadata": {},
   "source": [
    "##### BernouliNB is better for Binary classification in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "743c4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBM = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "70a5852a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBM.fit(Final_tf.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "60b56a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = NBM.predict(Final_tf_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eb4ef3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b3a99c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[225  25]\n",
      " [  1 198]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(predicts,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158c12b",
   "metadata": {},
   "source": [
    "### RNN Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765b1f9",
   "metadata": {},
   "source": [
    "##### Word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c744510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['an apple a day keeps the doctor away and keeps your healthy',\n",
    "             'apple is looking to launch a new iphone this year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1693f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9bd330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saibh\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6bafca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [gensim.utils.simple_preprocess(i) for i in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "825ce9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['an', 'apple', 'day', 'keeps', 'the', 'doctor', 'away', 'and', 'keeps', 'your', 'healthy'], ['apple', 'is', 'looking', 'to', 'launch', 'new', 'iphone', 'this', 'year']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e0e5d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(window=5,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ac55df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(corpus,progress_per=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9216d61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1e80380c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fa4881ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 540)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences,epochs=model.epochs,total_examples=corpus.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "196d9f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0,\n",
       " 'keeps': 1,\n",
       " 'year': 2,\n",
       " 'this': 3,\n",
       " 'day': 4,\n",
       " 'the': 5,\n",
       " 'doctor': 6,\n",
       " 'away': 7,\n",
       " 'and': 8,\n",
       " 'your': 9,\n",
       " 'healthy': 10,\n",
       " 'is': 11,\n",
       " 'looking': 12,\n",
       " 'to': 13,\n",
       " 'launch': 14,\n",
       " 'new': 15,\n",
       " 'iphone': 16,\n",
       " 'an': 17}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cba96d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = model.wv.get_vector('iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a000f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = model.wv.get_vector('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6683b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector3 = model.wv.get_vector('doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "46b3962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "592fe5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21883951]], dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([vector1],[vector2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "77e1a462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01613472]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([vector2],[vector3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5e85864b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16378775]], dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([vector1],[vector3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69c925",
   "metadata": {},
   "source": [
    "##### Embedding Layer Technique to get the Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58943387",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_final['clean_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ef6d447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174                                        see i thought\n",
       "211                                      home always chat\n",
       "5146    oh unintentionally bad timing great fingers tr...\n",
       "3119                               good evening roger how\n",
       "1669    very hurting n meaningful lines ever \\i compro...\n",
       "                              ...                        \n",
       "5537    want explicit sex  secs ring  costs pmin gsex ...\n",
       "5540    asked mobile if  chatlines inclu in free mins ...\n",
       "5547    had contract mobile  mnths latest motorola nok...\n",
       "5566    reminder from o to get  pounds free call credi...\n",
       "5567    this nd time tried  contact u u  pound prize  ...\n",
       "Name: clean_messages, Length: 1494, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e9100a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce7faf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1494"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12e61187",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa23d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "369e8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "    tokens = nltk.word_tokenize(i)\n",
    "    \n",
    "    if maxi < len(tokens):\n",
    "        maxi = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a52da5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ec49f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "eg1 = ['boy is good','girl is good']\n",
    "eg2 = ['good is glass','girl is also good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d91a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_repr1 = [one_hot(i,50) for i in eg1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c2c48eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_repr2 = [one_hot(i,50) for i in eg2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aea80aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[36, 23, 41], [2, 23, 41]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_repr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fadd3105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[41, 23, 5], [2, 23, 4, 41]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_repr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7b98db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=[  'the glass of milk',\n",
    "     'the glass of juice',\n",
    "     'the cup of tea',\n",
    "    'I am a good boy',\n",
    "     'I am a good developer',\n",
    "     'understand the meaning of words',\n",
    "     'your videos are good',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7830345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_rep = [one_hot(i,50) for i in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "060ccd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 5, 43, 49],\n",
       " [10, 5, 43, 15],\n",
       " [10, 49, 43, 22],\n",
       " [17, 49, 29, 41, 36],\n",
       " [17, 49, 29, 41, 47],\n",
       " [38, 10, 26, 43, 18],\n",
       " [20, 47, 37, 41]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f812aa",
   "metadata": {},
   "source": [
    "##### We cab see that for one hot_repr for a same vocab_size and in the same kernel the words are geting same token number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ab50d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1494"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['clean_messages'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "509bb6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message           747\n",
       "class             747\n",
       "clean_messages    747\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['class']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39c10b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87c6b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df_final['clean_messages'],df_final['class'],test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e60ac",
   "metadata": {},
   "source": [
    "##### One_hot_repr of X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9240db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_repr_xtrain = [one_hot(i,vocab_size) for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7721d5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_hot_repr_xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63284ce",
   "metadata": {},
   "source": [
    "##### Pad the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ca973aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4246f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "282653d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train = pad_sequences(one_hot_repr_xtrain,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8d17f6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,   94, 2466, 1985,\n",
       "       1220, 8374,  542, 1771, 2466, 7050, 1139, 7860, 7193, 1139, 8374,\n",
       "       7555, 7860, 7969, 5300, 6048])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train[0]#consider it as a vector of dimension 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "756141f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1045, 82)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(padded_X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "90bb6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,SimpleRNN,Dense,Dropout,Input\n",
    "from keras.models import Sequential,Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7d536a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size,50,input_length=max_len))\n",
    "model1.compile('adam','mse')\n",
    "embedded_vectors = model1.predict(padded_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4cdd71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_vectors_eg = embedded_vectors.reshape(1045,50,82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "82edba9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1045, 82, 50)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3b2147ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(128,input_shape=(50,82),activation='relu',return_sequences=True))\n",
    "model.add(SimpleRNN(128,activation='relu'))\n",
    "model.add(Dense(32,'relu'))\n",
    "model.add(Dense(1,'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "80fd60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',metrics='accuracy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cb28e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_18 (SimpleRNN)    (None, 50, 128)           27008     \n",
      "_________________________________________________________________\n",
      "simple_rnn_19 (SimpleRNN)    (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 64,065\n",
      "Trainable params: 64,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "581412d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 - 2s - loss: 0.6150 - accuracy: 0.6766\n",
      "Epoch 2/50\n",
      "33/33 - 1s - loss: 0.4858 - accuracy: 0.7952\n",
      "Epoch 3/50\n",
      "33/33 - 1s - loss: 0.4429 - accuracy: 0.8191\n",
      "Epoch 4/50\n",
      "33/33 - 1s - loss: 0.4048 - accuracy: 0.8316\n",
      "Epoch 5/50\n",
      "33/33 - 1s - loss: 0.3390 - accuracy: 0.8660\n",
      "Epoch 6/50\n",
      "33/33 - 1s - loss: 0.2672 - accuracy: 0.9062\n",
      "Epoch 7/50\n",
      "33/33 - 1s - loss: 0.2099 - accuracy: 0.9215\n",
      "Epoch 8/50\n",
      "33/33 - 1s - loss: 0.1296 - accuracy: 0.9598\n",
      "Epoch 9/50\n",
      "33/33 - 1s - loss: 0.1054 - accuracy: 0.9627\n",
      "Epoch 10/50\n",
      "33/33 - 1s - loss: 0.0863 - accuracy: 0.9751\n",
      "Epoch 11/50\n",
      "33/33 - 1s - loss: 0.0629 - accuracy: 0.9799\n",
      "Epoch 12/50\n",
      "33/33 - 1s - loss: 0.0561 - accuracy: 0.9837\n",
      "Epoch 13/50\n",
      "33/33 - 1s - loss: 0.0428 - accuracy: 0.9885\n",
      "Epoch 14/50\n",
      "33/33 - 1s - loss: 0.0532 - accuracy: 0.9847\n",
      "Epoch 15/50\n",
      "33/33 - 1s - loss: 0.0395 - accuracy: 0.9904\n",
      "Epoch 16/50\n",
      "33/33 - 1s - loss: 0.0449 - accuracy: 0.9837\n",
      "Epoch 17/50\n",
      "33/33 - 1s - loss: 0.0398 - accuracy: 0.9885\n",
      "Epoch 18/50\n",
      "33/33 - 1s - loss: 0.0200 - accuracy: 0.9952\n",
      "Epoch 19/50\n",
      "33/33 - 1s - loss: 0.0177 - accuracy: 0.9943\n",
      "Epoch 20/50\n",
      "33/33 - 1s - loss: 0.0103 - accuracy: 0.9981\n",
      "Epoch 21/50\n",
      "33/33 - 1s - loss: 0.0136 - accuracy: 0.9952\n",
      "Epoch 22/50\n",
      "33/33 - 1s - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 23/50\n",
      "33/33 - 1s - loss: 0.0096 - accuracy: 0.9981\n",
      "Epoch 24/50\n",
      "33/33 - 1s - loss: 0.0170 - accuracy: 0.9943\n",
      "Epoch 25/50\n",
      "33/33 - 1s - loss: 0.0120 - accuracy: 0.9971\n",
      "Epoch 26/50\n",
      "33/33 - 1s - loss: 0.0046 - accuracy: 0.9981\n",
      "Epoch 27/50\n",
      "33/33 - 1s - loss: 0.0104 - accuracy: 0.9952\n",
      "Epoch 28/50\n",
      "33/33 - 1s - loss: 0.0051 - accuracy: 0.9981\n",
      "Epoch 29/50\n",
      "33/33 - 1s - loss: 0.0039 - accuracy: 0.9981\n",
      "Epoch 30/50\n",
      "33/33 - 1s - loss: 0.0058 - accuracy: 0.9990\n",
      "Epoch 31/50\n",
      "33/33 - 1s - loss: 0.0192 - accuracy: 0.9952\n",
      "Epoch 32/50\n",
      "33/33 - 1s - loss: 0.0445 - accuracy: 0.9837\n",
      "Epoch 33/50\n",
      "33/33 - 1s - loss: 0.0571 - accuracy: 0.9789\n",
      "Epoch 34/50\n",
      "33/33 - 1s - loss: 0.0557 - accuracy: 0.9837\n",
      "Epoch 35/50\n",
      "33/33 - 1s - loss: 0.0328 - accuracy: 0.9904\n",
      "Epoch 36/50\n",
      "33/33 - 1s - loss: 0.0204 - accuracy: 0.9923\n",
      "Epoch 37/50\n",
      "33/33 - 1s - loss: 0.0184 - accuracy: 0.9904\n",
      "Epoch 38/50\n",
      "33/33 - 1s - loss: 0.0041 - accuracy: 0.9990\n",
      "Epoch 39/50\n",
      "33/33 - 1s - loss: 0.0040 - accuracy: 0.9981\n",
      "Epoch 40/50\n",
      "33/33 - 1s - loss: 0.0034 - accuracy: 0.9981\n",
      "Epoch 41/50\n",
      "33/33 - 1s - loss: 0.0035 - accuracy: 0.9981\n",
      "Epoch 42/50\n",
      "33/33 - 1s - loss: 0.0069 - accuracy: 0.9981\n",
      "Epoch 43/50\n",
      "33/33 - 1s - loss: 0.0042 - accuracy: 0.9990\n",
      "Epoch 44/50\n",
      "33/33 - 1s - loss: 0.0022 - accuracy: 0.9990\n",
      "Epoch 45/50\n",
      "33/33 - 1s - loss: 0.0016 - accuracy: 0.9990\n",
      "Epoch 46/50\n",
      "33/33 - 1s - loss: 0.0023 - accuracy: 0.9990\n",
      "Epoch 47/50\n",
      "33/33 - 1s - loss: 9.6423e-04 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "33/33 - 1s - loss: 0.0017 - accuracy: 0.9990\n",
      "Epoch 49/50\n",
      "33/33 - 1s - loss: 8.4003e-04 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "33/33 - 1s - loss: 6.6294e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22a63297820>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(embedded_vectors_eg,y_train,epochs=50,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "950368f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_repr_xtest = [one_hot(i,vocab_size) for i in X_test]\n",
    "padded_xtest = pad_sequences(one_hot_repr_xtest,82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a90c659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_vectors_test = model1.predict(padded_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6a682173",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_vectors_test_eg = embedded_vectors_test.reshape(embedded_vectors_test.shape[0],50,82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "642bb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(embedded_vectors_test_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "44ca8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "834018d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predictions:\n",
    "    if i>0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "112999d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3a6e0e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692     0\n",
       "4724    0\n",
       "159     1\n",
       "5129    0\n",
       "800     1\n",
       "       ..\n",
       "3585    1\n",
       "1427    0\n",
       "2577    0\n",
       "2334    0\n",
       "203     0\n",
       "Name: class, Length: 449, dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "48658f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "96904a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[190,  36],\n",
       "       [ 34, 189]], dtype=int64)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "33010a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 9ms/step - loss: 1.1786 - accuracy: 0.8441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1786072254180908, 0.8440979719161987]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(embedded_vectors_test_eg,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f0368",
   "metadata": {},
   "source": [
    "#### BERT ENCODING AND SENTIMENT ANALYSIS IN ANOTHER NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
